Implementing an RL-DDM for the Emotional Face Task in RxInfer

GitHub code of the simulation:


Hello RxInfer community,

I’m working on a two-hundred-trial “emotional face” experiment in which a participant hears a low or high tone, then sees a face whose expression varies from clearly sad to clearly angry. They must decide “sad” or “angry” as quickly as possible; correct, fast responses yield higher reward, while incorrect or too-slow responses incur minimal reward. The tone–face association changes unpredictably during the session, so the model must continuously relearn which tone maps to which emotion.

Model structure and code (in Julia):
 • Reinforcement learner
  • Two Q-values, one for each possible mapping (high-angry/low-sad vs. high-sad/low-angry)
  • Learning rates η_win, η_loss and forgetting factor ω
  • Softmax inverse temperature β for action selection
 • DDM decision process
  • Base drift rate v, boundary separation a, non-decision time T, starting bias w
  • Drift rate on trial t: vₜ = v + k_tone × intensityₜ
  • Boundary separation modulated by belief difference: aₜ = a + ρ·(Q₁ – Q₂)
 • Reward
  • If RT > 0.8 s: “missing” reward
  • If correct: linear mapping from RT to reward between [–0.25, 0.75]
  • If incorrect: fixed penalty
 • Simulation code attaches these pieces, samples via DDM(vₜ, aₜ, w, T) from SequentialSamplingModels.jl, and updates the Q-values with a Rescorla–Wagner rule. See full code here:GitHub code of the simulation



My questions:
 1. Model specification
 • How do I declare a custom joint model that combines an RL update with a DDM observation model in RxInfer’s GraphPPL syntax?
 • Is there a recommended pattern for embedding a drift-diffusion process inside an inference graph?
 2. Parameter estimation
 • Which RxInfer sampler or variational method works best when part of the generative model includes a sequential sampler like DDM?
 • Are there example notebooks showing how to fit a SequentialSamplingModels.jl distribution within RxInfer?
 3. Data interface
 • Best practice for feeding trial-by-trial arrays (tone, intensity, response, RT, reward) into the inference routine?
 • How to handle discrete Q-value updates within a probabilistic program—should they be treated as latent state variables, or externally computed?

Any pointers to existing examples, patterns, or code snippets would be greatly appreciated. Thank you in advance for your guidance!